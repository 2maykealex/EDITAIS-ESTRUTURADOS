

APRENDIZADO DE MÁQUINA: 

1 Técnicas de classificação. 1.1 Naive Bayes. 1.2 Regressão logística. 1.3 Redes
neurais artificiais. 1.3.1 Funções de ativação: limiar, linear, ReLU, logística, softmax, maxout e gaussiana. 1.3.2
Redes Perceptron de única e múltiplas camadas. 1.4 Árvores de decisão (algoritmos ID3 e C4.5) e florestas
aleatórias (random forest). 1.5 Máquinas de vetores de suporte (SVM – support vector machines). 1.6 K
vizinhos mais próximos (KNN – K-nearest neighbors). 1.7 Comitês de classificadores. 1.8 Avaliação de modelos
de classificação: treinamento/teste/validação; validação cruzada; métricas de avaliação (matriz de confusão,
acurácia, precisão, revocação, F1-score e curva ROC). 2 Técnicas de regressão. 2.1 Regressão linear. 2.2 Séries
temporais (tendências, suavização exponencial e modelos ARIMA). 2.3 Redes neurais para regressão. 2.4
Árvores de decisão para regressão. 2.5 Máquinas de vetores de suporte para regressão. 2.6 Intervalos de
confiança em regressão. 2.7 Avaliação de modelos de regressão: mean absolute error (MAE), mean square
error (MSE), root mean square error (RMSE) e coeficiente de determinação (R2). 

3 Técnicas de agrupamento.
3.1 Agrupamento por partição. 3.2 Agrupamento por densidade. 3.3 Agrupamento hierárquico. 4 Técnicas de
redução de dimensionalidade. 4.1 Seleção de características (feature selection). 4.2 Análise de componentes
principais (PCA – principal component analysis). 5 Técnicas de associação. 5.1 Descoberta de conjuntos
frequentes. 5.2 Descoberta de regras de associação. 6 Sistemas de recomendação. 7 Processamento de
linguagem natural (PLN). 7.1 Normalização textual (stop words, estemização, lematização e análise de
frequência de termos). 7.2 Rotulação de partes do discurso (POS-tagging – part-of-speech tagging). 7.3
Reconhecimento de entidades (NER – named entity recognition) e rotulação IOB. 7.4 Modelos de
representação de texto: N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos
vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector). 7.5
Métricas de similaridade textual (similaridade do cosseno, distância euclidiana, similaridade de Jaccard,
distância de Manhattan e coeficiente de Dice). 7.6 Aplicações de PLN: sumarização automática de texto
(abordagens extrativa e abstrativa), modelagem de tópicos em texto (algoritmos LSI, LDA e NMF),
classificação de texto, agrupamento de texto, tradução automática de texto, análise de sentimentos e
emoções em texto, reconhecimento de voz (STT – speech to text). 8 Visão computacional. 8.1
Reconhecimento facial. 8.2 Classificação de imagens. 8.3 Detecção de objetos. 8.4 Deep learning para visão
computacional. 9 Aprendizado profundo. 9.1 Redes neurais convolucionais. 9.2 Redes neurais recorrentes.
9.2.1 Redes de Hopfield. 9.2.2 Long short-term memory (LSTM). 9.2.3 Redes perceptron multicamadas
recorrentes. 9.2.4 Máquinas de Boltzmann. 9.2.5 Deep belief networks.